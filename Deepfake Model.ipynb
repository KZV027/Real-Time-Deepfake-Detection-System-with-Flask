{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "544327b3-3778-4685-815e-ed0e638a7ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\kshit\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\kshit\\anaconda3\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\kshit\\anaconda3\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: Pillow in c:\\users\\kshit\\anaconda3\\lib\\site-packages (10.3.0)\n",
      "Collecting pickle5\n",
      "  Downloading pickle5-0.0.11.tar.gz (132 kB)\n",
      "     ---------------------------------------- 0.0/132.1 kB ? eta -:--:--\n",
      "     ----- ------------------------------- 20.5/132.1 kB 330.3 kB/s eta 0:00:01\n",
      "     ----------------- ------------------- 61.4/132.1 kB 544.7 kB/s eta 0:00:01\n",
      "     -------------------------------------- 132.1/132.1 kB 1.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.66.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.5.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\kshit\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.0)\n",
      "Building wheels for collected packages: pickle5\n",
      "  Building wheel for pickle5 (setup.py): started\n",
      "  Building wheel for pickle5 (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for pickle5\n",
      "Failed to build pickle5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [17 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-312\n",
      "  creating build\\lib.win-amd64-cpython-312\\pickle5\n",
      "  copying pickle5\\pickle.py -> build\\lib.win-amd64-cpython-312\\pickle5\n",
      "  copying pickle5\\pickletools.py -> build\\lib.win-amd64-cpython-312\\pickle5\n",
      "  copying pickle5\\__init__.py -> build\\lib.win-amd64-cpython-312\\pickle5\n",
      "  creating build\\lib.win-amd64-cpython-312\\pickle5\\test\n",
      "  copying pickle5\\test\\pickletester.py -> build\\lib.win-amd64-cpython-312\\pickle5\\test\n",
      "  copying pickle5\\test\\test_pickle.py -> build\\lib.win-amd64-cpython-312\\pickle5\\test\n",
      "  copying pickle5\\test\\test_picklebuffer.py -> build\\lib.win-amd64-cpython-312\\pickle5\\test\n",
      "  copying pickle5\\test\\__init__.py -> build\\lib.win-amd64-cpython-312\\pickle5\\test\n",
      "  running build_ext\n",
      "  building 'pickle5._pickle' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for pickle5\n",
      "ERROR: Could not build wheels for pickle5, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "pip install numpy tensorflow opencv-python Pillow pickle5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2903b1d8-e51c-4cf8-8804-04cba008cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf94e128-f78e-4d3e-9ef7-01ed02453755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 140002 images belonging to 2 classes.\n",
      "Found 39428 images belonging to 2 classes.\n",
      "Found 10905 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Define the paths for your dataset\n",
    "train_directory = r'C:\\Users\\kshit\\Downloads\\archive\\Dataset\\Train'\n",
    "validation_directory = r'C:\\Users\\kshit\\Downloads\\archive\\Dataset\\Validation'\n",
    "test_directory = r'C:\\Users\\kshit\\Downloads\\archive\\Dataset\\Test'\n",
    "\n",
    "# ImageDataGenerator for loading images from the train, validation, and test directories\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Flow images in batches from the directory for training\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_directory,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'  # Assuming binary classification: 'real' and 'fake'\n",
    ")\n",
    "\n",
    "# Flow images in batches from the directory for validation\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_directory,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Flow images in batches from the directory for testing\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_directory,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Define the model (already done earlier in the code)\n",
    "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0fd2302-6683-4664-85aa-d25f0ff403bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kshit\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4376/4376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716ms/step - accuracy: 0.8111 - loss: 0.3987\n",
      "Epoch 1: val_accuracy improved from -inf to 0.83512, saving model to best_model.keras\n",
      "\u001b[1m4376/4376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4055s\u001b[0m 925ms/step - accuracy: 0.8111 - loss: 0.3987 - val_accuracy: 0.8351 - val_loss: 0.3669\n",
      "Epoch 2/10\n",
      "\u001b[1m4376/4376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kshit\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n",
      "C:\\Users\\kshit\\anaconda3\\Lib\\site-packages\\keras\\src\\callbacks\\early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "  current = self.get_monitor_value(logs)\n",
      "C:\\Users\\kshit\\anaconda3\\Lib\\site-packages\\keras\\src\\callbacks\\model_checkpoint.py:206: UserWarning: Can save best model only with val_accuracy available, skipping.\n",
      "  self._save_model(epoch=epoch, batch=None, logs=logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4376/4376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525ms/step - accuracy: 0.8559 - loss: 0.3232\n",
      "Epoch 3: val_accuracy improved from 0.83512 to 0.84260, saving model to best_model.keras\n",
      "\u001b[1m4376/4376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2936s\u001b[0m 671ms/step - accuracy: 0.8559 - loss: 0.3232 - val_accuracy: 0.8426 - val_loss: 0.3556\n",
      "Epoch 4/10\n",
      "\u001b[1m4376/4376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "\u001b[1m4376/4376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529ms/step - accuracy: 0.8665 - loss: 0.3017\n",
      "Epoch 5: val_accuracy did not improve from 0.84260\n",
      "\u001b[1m4376/4376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2958s\u001b[0m 676ms/step - accuracy: 0.8665 - loss: 0.3017 - val_accuracy: 0.8424 - val_loss: 0.3510\n",
      "Epoch 6/10\n",
      "\u001b[1m4376/4376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "\u001b[1m4376/4376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567ms/step - accuracy: 0.8848 - loss: 0.2685\n",
      "Epoch 7: val_accuracy improved from 0.84260 to 0.85160, saving model to best_model.keras\n",
      "\u001b[1m4376/4376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3146s\u001b[0m 719ms/step - accuracy: 0.8848 - loss: 0.2685 - val_accuracy: 0.8516 - val_loss: 0.3355\n",
      "Epoch 8/10\n",
      "\u001b[1m4376/4376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "\u001b[1m4376/4376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526ms/step - accuracy: 0.8960 - loss: 0.2468\n",
      "Epoch 9: val_accuracy improved from 0.85160 to 0.85957, saving model to best_model.keras\n",
      "\u001b[1m4376/4376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2958s\u001b[0m 676ms/step - accuracy: 0.8960 - loss: 0.2468 - val_accuracy: 0.8596 - val_loss: 0.3244\n",
      "Epoch 10/10\n",
      "\u001b[1m4376/4376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 804ms/step - accuracy: 0.7446 - loss: 0.7252\n",
      "Test Accuracy: 74.29%\n"
     ]
    }
   ],
   "source": [
    "# Early stopping and model checkpoint to save the best model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "checkpoint = ModelCheckpoint('best_model.keras', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=10,  # Adjust the number of epochs as needed\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator),\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "evaluation = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {evaluation[1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb8108ed-053a-401d-b7a5-942fdc42990f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the final model weights\n",
    "model.save('final_deepfake_model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "313fec29-a4a0-4f98-8bb1-631ccd1b0e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10905 images belonging to 2 classes.\n",
      "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 511ms/step - accuracy: 0.7440 - loss: 0.7213\n",
      "Test Accuracy: 74.29%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_generator = validation_datagen.flow_from_directory(\n",
    "    test_directory,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f193b226-48e4-43ae-8906-471004b35f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model architecture and weights using Pickle\n",
    "with open('deepfake_model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "# Saving the model weights separately using the correct .weights.h5 extension\n",
    "model.save_weights('deepfake_model_weights.weights.h5')  # Changed to .weights.h5\n",
    "\n",
    "# Loading the model and weights in the future\n",
    "# with open('deepfake_model.pkl', 'rb') as file:\n",
    "#     loaded_model = pickle.load(file)\n",
    "# loaded_model.load_weights('deepfake_model_weights.weights.h5')  # Load the .weights.h5 weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b9b97f-c57a-410c-be6d-f48550e476f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
